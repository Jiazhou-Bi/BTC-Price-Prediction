{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Preamble ####\n",
    "# Purpose: Cleans, engineers, and saves the cleaned data \n",
    "# Author: Jiazhou(Justin) Bi\n",
    "# Date: 15 Nov 2024\n",
    "# Contact: justin.bi@mail.utoronto.ca\n",
    "# License: None\n",
    "# Pre-requisites: see requirements.txt\n",
    "# Any other information needed? None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the datasets as DataFrame\n",
    "df_1m = pd.read_parquet('../data/01-raw_data/raw_data_1m.parquet')\n",
    "df_1h = pd.read_parquet('../data/01-raw_data/raw_data_1h.parquet')\n",
    "df_1d = pd.read_parquet('../data/01-raw_data/raw_data_1d.parquet')\n",
    "# print(df_1m.head())\n",
    "# print(df_1h.head())\n",
    "# print(df_1d.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp    0\n",
       "open         0\n",
       "high         0\n",
       "low          0\n",
       "close        0\n",
       "volume       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1m.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp    0\n",
       "open         0\n",
       "high         0\n",
       "low          0\n",
       "close        0\n",
       "volume       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1h.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp    0\n",
       "open         0\n",
       "high         0\n",
       "low          0\n",
       "close        0\n",
       "volume       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1d.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values found in these datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section examines the datatypes of each column and ensures they are appropriate for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp    datetime64[ns]\n",
       "open                float64\n",
       "high                float64\n",
       "low                 float64\n",
       "close               float64\n",
       "volume              float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1m.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp    datetime64[ns]\n",
       "open                float64\n",
       "high                float64\n",
       "low                 float64\n",
       "close               float64\n",
       "volume              float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1h.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp    datetime64[ns]\n",
       "open                float64\n",
       "high                float64\n",
       "low                 float64\n",
       "close               float64\n",
       "volume              float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1d.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is to validate if the datasets are consecutive. That is, for the 1-minute timestamp dataset, no minutes should be skipped. Same logic appleis to the 1-hour and 1-day timestamp datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating if any minute is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a full range of minutes for the 1-minute dataset\n",
    "full_minute_range = pd.date_range(start=df_1m['timestamp'].min(), end=df_1m['timestamp'].max(), freq='1min')\n",
    "# print(full_minute_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reindexing the DataFrame\n",
    "df_1m.set_index('timestamp', inplace=True)\n",
    "df_1m = df_1m.reindex(full_minute_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with missing values:\n",
      "                     open  high  low  close  volume\n",
      "2017-09-06 16:01:00   NaN   NaN  NaN    NaN     NaN\n",
      "2017-09-06 16:02:00   NaN   NaN  NaN    NaN     NaN\n",
      "2017-09-06 16:03:00   NaN   NaN  NaN    NaN     NaN\n",
      "2017-09-06 16:04:00   NaN   NaN  NaN    NaN     NaN\n",
      "2017-09-06 16:05:00   NaN   NaN  NaN    NaN     NaN\n",
      "...                   ...   ...  ...    ...     ...\n",
      "2023-03-24 13:55:00   NaN   NaN  NaN    NaN     NaN\n",
      "2023-03-24 13:56:00   NaN   NaN  NaN    NaN     NaN\n",
      "2023-03-24 13:57:00   NaN   NaN  NaN    NaN     NaN\n",
      "2023-03-24 13:58:00   NaN   NaN  NaN    NaN     NaN\n",
      "2023-03-24 13:59:00   NaN   NaN  NaN    NaN     NaN\n",
      "\n",
      "[8632 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values across the entire DataFrame\n",
    "missing_rows = df_1m[df_1m.isna().any(axis=1)]\n",
    "\n",
    "if missing_rows.empty:\n",
    "    print(\"No missing values.\")\n",
    "else:\n",
    "    print(\"Rows with missing values:\")\n",
    "    print(missing_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are missing minutes. We will first mark the missing rows down in a new column called 'was_miss'. 1 indicates the row was empty and 0 means not. Then the missing values are filled with linear interpolation, except the column \"volume\", which is replaced with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an indicator to flag the missing rows\n",
    "df_1m['was_missing'] = df_1m.isna().any(axis=1).astype(int)\n",
    "\n",
    "# Interpolate missing values\n",
    "df_1m.interpolate(method='linear', inplace=True)\n",
    "df_1m.loc[df_1m['was_missing'] == 1, 'volume'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values.\n"
     ]
    }
   ],
   "source": [
    "# Check agian for missing values across the entire DataFrame\n",
    "missing_rows = df_1m[df_1m.isna().any(axis=1)]\n",
    "\n",
    "if missing_rows.empty:\n",
    "    print(\"No missing values.\")\n",
    "else:\n",
    "    print(\"Rows with missing values:\")\n",
    "    print(missing_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating if any hour is missing from the 1-hour dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with missing values:\n",
      "                     open  high  low  close  volume\n",
      "2017-09-06 17:00:00   NaN   NaN  NaN    NaN     NaN\n",
      "2017-09-06 18:00:00   NaN   NaN  NaN    NaN     NaN\n",
      "2017-09-06 19:00:00   NaN   NaN  NaN    NaN     NaN\n",
      "2017-09-06 20:00:00   NaN   NaN  NaN    NaN     NaN\n",
      "2017-09-06 21:00:00   NaN   NaN  NaN    NaN     NaN\n",
      "...                   ...   ...  ...    ...     ...\n",
      "2021-08-13 04:00:00   NaN   NaN  NaN    NaN     NaN\n",
      "2021-08-13 05:00:00   NaN   NaN  NaN    NaN     NaN\n",
      "2021-09-29 07:00:00   NaN   NaN  NaN    NaN     NaN\n",
      "2021-09-29 08:00:00   NaN   NaN  NaN    NaN     NaN\n",
      "2023-03-24 13:00:00   NaN   NaN  NaN    NaN     NaN\n",
      "\n",
      "[128 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# using the same logic as above\n",
    "full_hour_range = pd.date_range(start=df_1h['timestamp'].min(), end=df_1h['timestamp'].max(), freq='1h')\n",
    "df_1h.set_index('timestamp', inplace=True)\n",
    "df_1h = df_1h.reindex(full_hour_range)\n",
    "\n",
    "# Check for missing values across the entire DataFrame\n",
    "missing_rows_1h = df_1h[df_1h.isna().any(axis=1)]\n",
    "\n",
    "if missing_rows_1h.empty:\n",
    "    print(\"No missing values.\")\n",
    "else:\n",
    "    print(\"Rows with missing values:\")\n",
    "    print(missing_rows_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values.\n"
     ]
    }
   ],
   "source": [
    "# applying the same logic as above\n",
    "df_1h['was_missing'] = df_1h.isna().any(axis=1).astype(int)\n",
    "\n",
    "# Interpolate missing values\n",
    "df_1h.interpolate(method='linear', inplace=True)\n",
    "df_1h.loc[df_1h['was_missing'] == 1, 'volume'] = 0\n",
    "\n",
    "# Check agian for missing values across the entire DataFrame\n",
    "missing_rows_1h = df_1h[df_1h.isna().any(axis=1)]\n",
    "\n",
    "if missing_rows_1h.empty:\n",
    "    print(\"No missing values.\")\n",
    "else:\n",
    "    print(\"Rows with missing values:\")\n",
    "    print(missing_rows_1h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating if any day is missing from the 1-day dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values.\n"
     ]
    }
   ],
   "source": [
    "# using the same logic as above\n",
    "full_day_range = pd.date_range(start=df_1d['timestamp'].min(), end=df_1d['timestamp'].max(), freq='1d')\n",
    "df_1d.set_index('timestamp', inplace=True)\n",
    "df_1d = df_1d.reindex(full_day_range)\n",
    "\n",
    "# Check for missing values across the entire DataFrame\n",
    "missing_rows_1d = df_1d[df_1d.isna().any(axis=1)]\n",
    "\n",
    "if missing_rows_1d.empty:\n",
    "    print(\"No missing values.\")\n",
    "else:\n",
    "    print(\"Rows with missing values:\")\n",
    "    print(missing_rows_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the 1-day dataset, no days were missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding A Column For Price Change Direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This subsection creates a new column for each dataset called direction. If the closing price is higher than the previous closing price, it is considered that the price has gone up and thus marked as 1 for appreciation. If the closing price is lower than the previous closing price, it is considered that the price has gone down and hence is marked as -1 for depreciation. If the price remains the same,  it is marked as 0 for no movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculting the direction\n",
    "df_1m['direction'] = df_1m['close'].diff().apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
    "\n",
    "# Dropping the first row as it does not have a direction\n",
    "df_1m.reset_index(inplace=True)\n",
    "df_1m = df_1m.iloc[1:].reset_index(drop=True)\n",
    "#df_1m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the same logic to the 1-hour and 1-day datasets\n",
    "df_1h['direction'] = df_1h['close'].diff().apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
    "df_1h.reset_index(inplace=True)\n",
    "df_1h = df_1h.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "df_1d['direction'] = df_1d['close'].diff().apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
    "df_1d.reset_index(inplace=True)\n",
    "df_1d = df_1d.iloc[1:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the DataFrame as a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1m.to_parquet('../data/02-analysis_data/cleaned_data_1m.parquet', index=False)\n",
    "df_1h.to_parquet('../data/02-analysis_data/cleaned_data_1h.parquet', index=False)\n",
    "df_1d.to_parquet('../data/02-analysis_data/cleaned_data_1d.parquet', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
