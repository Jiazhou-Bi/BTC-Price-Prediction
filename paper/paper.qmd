---
title: "Comparing Different Machine Learning Models at Predicting Bitcoin Price Moving Direction at Different Time Intervals from 2017 to 2024"
subtitle: "RNN Can Predict Best at One-Hour Intervals and XGB Trains the Fastest"
author: 
  - Justin (Jiazhou) Bi
thanks: "All project related files available at: https://github.com/Jiazhou-Bi/BTC-Price-Prediction"
date: Dec 7, 2024
date-format: long
abstract: "This project aims to build a predictive model for the price moving direction of Bitcoin (BTC) at 1-minute, 1-hour, and 1-day intervals. We explored four popular machine learning algorithms for time-series data prediction: Recurrent Neural Network (RNN), Long Short-Term Memory (LSTM), Random Forest (RF), and Extreme Gradient Boosting (xGB). We have compared these four models on two different dimensions: model performance and time for training. The data used for this project is gathered from Binance which contains BTC data from late 2017 to present (November 2024).The original variables include high, low, open, close, and volume, as well as the timestamp of BTC, at one-minute, one-hour, and one-day intervals. Ultimately, RNN has the overall best performance in terms of its predictive accuracy, while XGB has the fastest training duration. While most models across different time intervals achieved a better performance than simply guessing at a 50-50 percent chance, their performance varies for each direction of the movemenet. Future steps for improving the model may involve incorporating additional features to enhance model accuracy and reduce errors. Additionally, fine-tuning hyperparameters, experimenting with more sophisticated feature engineering, and handling outliers could further improve performance."

format: pdf
jupyter: python3 
number-sections: true
bibliography: references.bib
---

# Introduction {#sec-introduction}

Cryptocurrencies have drawn much attention from the public recently due to their growing public awareness. Researchers have found that 4,495 pieces of literature related to Bitcoin were published from 2011 to 2020 [@aysan2021ascent]. More recently, another study stated that more than 9,100 research papers were published on cryptocurrencies in the Web of Science [@bara2024impact]. In 2015, a journal called Ledger was published by the University of Pittsburgh's University Library System, and it is the first journal dedicated to cryptocurrencies [@extance2015bitcoin]. This reflects the trend of growing interest in digital assets. In December 2019, there were 2,813 different cryptocurrencies existing with 201.47 billion US dollars in their total market capitalization; in particular, bitcoin is the most significant contributor and had a 131.89 billion US dollars market capitalization [@chen2021empirical]. When writing this article on December 4, 2024, the current market capitalization of Bitcoin is 1.95 trillion US dollars, with a 103.22 billion US dollars 24-hour trading volume, according to forbes.com [@Forbes]. As of October 2023, there were a total of 1492 cryptocurrency trading platforms globally, both centralized and decentralized [@Oh_2024]. Among all the different cryptocurrencies, Bitcoin is the key player in the market and the research field [@aysan2021ascent]. Therefore, learning more about Bitcoin can benefit both researchers and the public interested in cryptocurrency and its trade.

On the other hand, volatility is a key aspect of Bitcoin due to its non-tangible nature. Much research has focused on the price fluctuations of Bitcoin and compared its volatility to more traditional assets, such as gold. For example, [@kurihara2018does] demonstrated that Bitcoin has higher volatility than gold, indicating its speculative nature. In another study, the authors have concluded that Bitcoin's extreme price volatility may interfere with its use as a stable investment instrument. As a result, speculative investors are often more attracted to Bitcoin [@chen2021empirical] and are looking for profit by capitalizing on its rapid price changes [@baur2018bitcoin].

Despite its extreme volatility, Bitcoin is still widely accepted as a tradable investment vehicle, as many characteristics are similar to stocks [@chen2021empirical]. In a recently published study, the authors developed a model called TradExpert-NM, which can achieve a 59% accuracy for Bitcoin's daily price movement direction [@ding2024tradexpert]. In another study, [@mudassir2020time] developed a model that can score up to 65% accuracy for the next-day forecast of Bitcoin's price movement. However, many of these "high-performing" models use various data (social media data in [@ding2024tradexpert] and other technical data [@mudassir2020time]) that are usually too complicated for investors with limited knowledge of machine learning and data science. Even for models that used only more accessible data, such as in [@adcock2019non], they utilized neural networks as their model. However, deep learning models, like deep neural networks, generally require more data than traditional machine learning models and can take longer to train [@janiesch2021machine]. Therefore, these models pose challenges for individuals who want to start trading Bitcoin with limited knowledge and resources. As a result, in this project, three commonly used models for price prediction are compared on their performance and relative training effort by using only commonly available, easy-to-understand features (i.e., High Price, Low Price, Open Price, Close Price, and Trade Volume). This project aims to provide some knowledge for beginner crypto-traders who want to trade Bitcoin with some help from machine learning algorithms.

This project extracted data from Binance using a free Python [@python3] package called ccxt [@ccxt]. A total of three datasets were extracted. The three datasets contain the same time horizon, from August 17, 2017, to November 16, 2024. The only difference between them is their time intervals. The datasets were extracted at 1-minute, 1-hour, and 1-day intervals. Binance was chosen because it is the world's largest cryptocurrency exchange [@kowsmann202176], and it only offers data dating back to August 17, 2017, through its free API. Arguably, data before 2017 may be less important for our models' training because the cryptocurrency market was vastly different before 2017, and its price surged significantly in 2017 [@lim2020deep]. One potential drawback of this approach is that we are relying on the data from Binance as the single point of truth, which may not be the case across the whole time horizon in this case because the quality of the data is not cross-verified with other sources.

The ultimate goal of this project is to compare and contrast various machine learning models in terms of their technical performance and training effort in predicting Bitcoin's price movement direction at three different time intervals. By evaluating models on datasets with 1-minute, 1-hour, and 1-day intervals, this project aims to shed light on the trade-offs between predictive accuracy, computational efficiency, and accessibility for beginner traders. Furthermore, individuals with limited expertise in machine learning can leverage relatively simple and accessible data to make more informed, confident trading decisions by learning more about these models. In short, this project aims to empower crypto-traders by providing practical guidance on selecting and utilizing machine learning models effectively while balancing ease of use and predictive power.

In this analysis, we explored three different models: Long Short-Term Memory (LSTM), Recurrent Neural Network (RNN), and XGBoost (XGB) to predict Bitcoin's future price movement direction. LSTM and RNN are deep learning models that are excellent at handling sequential data. Thus, they are commonly used in price-prediction models. However, these two models generally need more computational resources, making their training/retraining processes effortful. On the other hand, XGB requires less computational effort thanks to its gradient-boosting framework, making it ideal for individuals with limited resources. By comparing these models on three different time intervals (1 minute, 1 hour, and 1 day), we aim to evaluate their predictive accuracy, training efficiency, and overall practicality for novice traders.

The remainder of this paper is structured as follows: @sec-data introduces the raw dataset, describes the cleaned datasets, and explains our final datasets with additionally engineered features, along with a preliminary analysis through numerical summaries and visualizations. @sec-models describes the three machine learning models being used in our study. @sec-results explores key findings from our analysis. Lastly, @sec-discussion addresses the limitations of the analysis and offers recommendations for future research projects.

# Data {#sec-data}

As mentioned in @sec-introduction, the datasets were extracted from Binance using a free Python [@python3] package called ccxt [@ccxt]. The extracted raw datasets contain the following variables: Timestamp, High, Low, Open, Close, and Volume. Two new variables were added to the cleaned datasets: direction_t-1 and direction_t+1. Lastly, in our final datasets, we have added four more variables: high_diff, low_diff, open_diff, and close_diff. Furthermore, some missing data were inputed for better data quality. Details of our data cleaning and engineering processes are provided in @sec-datacleaning and @sec-featureengineering. In the following subsections, we will review all the variables used in this report and provide some basic descriptive statistics. 

## Descriptive Data Analysis

In this subsection, we will explore every variable used in our study by examining some of the basic statistics. These preliminary analyses can help us better understand the data we are working with.

### Timestamp
The timestamp variable in our datasets is a key component for tracking the temporal aspect of Bitcoin's price movement. All three datasets start on August 17, 2017, and end on November 16, 2024. While ensuring all three datasets cover the same period, each dataset has different granularities. In the 1-minute dataset, the timestamp is recorded for each minute, providing the highest resolution data. In the 1-hour dataset, the timestamp is aggregated at hourly intervals. In the 1-day dataset, the timestamps are recorded at daily intervals. The differences in temporal granularity allow us to examine the trade-offs between datasets containing more information closely but may also increase computational complexity and noise. @tbl-table1 is an example of the timestamp variable from the 1-hour dataset.

```{python}

#| echo: false
#| label: tbl-table1
#| tbl-cap: "Example of the timestamp variable from the 1-hour dataset."
#| warning: false
#| message: false
#| fig-pos: "H"

import pandas as pd
data = pd.read_parquet('../data/01-raw_data/raw_data_1h.parquet')
df = data.drop(columns=['open', 'high', 'low', 'close', 'volume'])
df = df.set_index('timestamp')

df.head()

```

### Open
Open refers to the 'open price' of the tradable asset. As defined in [@chen2021empirical], the open price is the initial price of the cryptocurrency at the beginning of the trading session. For example, in the 1-hour dataset, the open price is the price of Bitcoin at the start of the hour. This is a commonly used indicator in the financial sector, and it shows the perceived value change of the underlying asset over time. @fig-figure1 reveals the change of the 1-hour open price of Bitcoin over the entire time horizon of the dataset.

```{python}

#| echo: false
#| label: fig-figure1
#| fig-cap: "Line chart of the open prices from the 1-hour dataset."
#| warning: false
#| message: false
#| fig-pos: "H"

import pandas as pd
import matplotlib.pyplot as plt
data = pd.read_parquet('../data/01-raw_data/raw_data_1h.parquet')

plt.figure(figsize=(10, 6))
plt.plot(data['timestamp'], data['open'], linewidth=0.5)
plt.title('Line Chart of Open Prices (1-Hour Interval)', fontsize=14)
plt.xlabel('Timestamp')
plt.ylabel('Open Price (USD)')
plt.ylim(0, None)
plt.grid(True)
plt.tight_layout()
plt.show()

```

### Close {#sec-close}
Similar to 'open,' 'close' refers to the 'close price' of the tradable asset. As defined in [@chen2021empirical], the close price is the last recorded price of Bitcoin before the trading session ends. For example, in the 1-minute dataset, the open price is the price of Bitcoin at the end of the minute. This variable is widely used in financial analysis as a key indicator of market sentiment and performance over a given time period. More importantly, close price is often considered the most robust indicator of the assets' perceived price, as suggested in [@Hayes]. Therefore, 'close' is also used as the 'final price' of Bitcoin in this project. @fig-figure2 shows the change of the 1-minute open price of Bitcoin over the entire time horizon of the dataset.

```{python}


#| echo: false
#| label: fig-figure2
#| fig-cap: "Line chart of the close prices from the 1-minute dataset."
#| warning: false
#| message: false
#| fig-pos: "H"

import pandas as pd
import matplotlib.pyplot as plt


data = pd.read_parquet('../data/01-raw_data/raw_data_1m.parquet')

# Plot the 'close' variable
plt.figure(figsize=(10, 6))
plt.plot(data['timestamp'], data['close'], linewidth=0.5)
plt.title('Line Chart of Close Prices (1-Minute Interval)', fontsize=14)
plt.xlabel('Timestamp')
plt.ylabel('Close Price (USD)')
plt.ylim(0, None)
plt.grid(True)
plt.tight_layout()
plt.show()

```

### High and Low {#sec-highandlow}
The high and low variables represent the extremes of Bitcoin's price within a given time interval. As suggested by the names, high refers to the highest price of Bitcoin during the time interval, and low refers to the lowest. [@lapitskaya2024prediction], [@poudel2023cryptocurrency], and many other researchers believe that incorporating basic features such as high and low prices can significantly improve the performance of cryptocurrency price prediction models. In @fig-figure3, both high and low from the 1-hour dataset are presented in the same graph. It is clear that these two are highly correlated, and this problem will be addressed later in @sec-featureengineering.

```{python}

#| echo: false
#| label: fig-figure3
#| fig-cap: "Line chart of high and low prices from the 1-hour dataset."
#| warning: false
#| message: false
#| fig-pos: "H"

import pandas as pd
import matplotlib.pyplot as plt


data = pd.read_parquet('../data/01-raw_data/raw_data_1h.parquet')
plt.figure(figsize=(10, 6))
plt.plot(data['timestamp'], data['high'], label='High Price', linewidth=0.5)
plt.plot(data['timestamp'], data['low'], label='Low Price', linewidth=0.5)
plt.title('Line Chart of High and Low Prices (1-Hour Interval)', fontsize=14)
plt.xlabel('Timestamp')
plt.ylabel('Price (USD)')
plt.ylim(0, None)
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

```

### Volume
The last raw variable extracted is volume. Volume represents the total amount of Bitcoin traded during a given time interval and indicates market activity and liquidity. For example, in the 1-hour dataset, the volume indicates the cumulative number of Bitcoins traded within an hour. In [@osina2023exploring], the authors have found that trade volume is an important feature when predicting the price of three different cryptocurrencies. Generally speaking, higher trading volume reflects increased market interest and is often associated with significant price fluctuations. As shown in @fig-figure4, the trading volume in the 1-day dataset varies significantly over time, illustrating periods of heightened trading activity.

```{python}

#| echo: false
#| label: fig-figure4
#| fig-cap: "Line chart of trading volume from the 1-day dataset."
#| warning: false
#| message: false
#| fig-pos: "H"

import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_parquet('../data/01-raw_data/raw_data_1d.parquet')

# Plot the 'volume' variable
plt.figure(figsize=(10, 6))
plt.plot(data['timestamp'], data['volume'], linewidth=0.5, color='orange')
plt.title('Line Chart of Trading Volume (1-Day Interval)', fontsize=14)
plt.xlabel('Timestamp')
plt.ylabel('Trading Volume (BTC)')
plt.ylim(0, None)
plt.grid(True)
plt.tight_layout()
plt.show()

```

## Data Cleaning {#sec-datacleaning}
There was no "obvious" missing value from the extracted datasets, as all the mentioned variables above are filled with either a timestamp in the "timestamp" column or a numeric value in all the remaining columns. However, upon a closer examination, the timestamps were not exhaustive. For example, in the 1-minute dataset, the timestamp "2017-09-06 16:01:00" does not exist even though it is included within the time horizon of the dataset. 8632 and 128 rows were missing from the 1-minute and 1-hour datasets, respectively. As a result, linear interpolation was used to estimate the missing values because it is a straightforward technique to fill in missing values in time series data [@moritz2015comparison]. Meanwhile, a new column was added to the 1-minute and 1-hour datasets to indicate if the prices of the row were either existing from the raw dataset or input. The new column is named "was_missing." The value of "was_missing" is 0 if the prices were extracted from the original data and 1 if the prices were filled with linear interpolation. Other than interpolating the prices of missing timestamps, no other data were manipulated.

## Feature Engineering {#sec-featureengineering}
First, two new features were added to indicate the price-moving direction of Bitcoin, namely "direction_t-1" and "direction_t+1". These features provide insights into the direction of Bitcoin's price change relative to the current timestamp. Second, four more new features were engineered to capture Bitcoin's price movements: "open_diff," "high_diff," "low_diff," and "close_diff." These features represent the lagged differences of the respective price variables (open, high, low, and close) from one timestamp to the next.

### Direction_t-1
This feature indicates the price movement direction compared to the previous timestamp. It was calculated by calculating the difference between the close price of the current row and the previous row. If the price has decreased from the previous timestamp, "direction_t-1" is marked as -1; if the close price has increased, then "direction_t-1" is marked as 1; if the close price did not change, it is assigned 0—the reason for choosing the close price as the final price indicator was explained in @sec-close.

### Direction_t+1
Similar to 'direction_t-1', 'direction_t+1' indicates the price movement direction at the next timestamp relative to the current close price, and it was calculated by calculating the close difference from the next timestamp to the current close price. "direction_t+1" is marked as -1 if the close price is lower in the next timestamp, and is marked as 1 if the close price is going up; if the close price does not change, it is assigned 0—the reason for choosing the close price as the final price indicator was explained in @sec-close. Because 'direction_t+1' indicates the price movement direction of the current timestamp, this variable is used as our models' outcome (i.e., the result the models try to predict).

### Open_Diff, High_Diff, Low_Diff, and Close_Diff
An issue was mentioned in @sec-highandlow. That is, the correlation between the price variables (open, high, low, and close) is too high (almost or equal to 1), as illustrated below in @fig-figure5 and @fig-figure6.

```{python}

#| echo: false
#| label: fig-figure5
#| fig-cap: "All price variables (open, high, low, close) plotted for the 1-minute dataset."
#| warning: false
#| message: false


import matplotlib.pyplot as plt
import matplotlib.image as mpimg

img = mpimg.imread('../other/graphs/all_price_plot_1m.png')
plt.figure(figsize=(10, 6))
plt.imshow(img)
plt.axis('off')
plt.show()

```

```{python}

#| echo: false
#| label: fig-figure6
#| fig-cap: "Correlation matrix (open, high, low, close) plotted for the 1-minute dataset."
#| warning: false
#| message: false


import matplotlib.pyplot as plt
import matplotlib.image as mpimg

img = mpimg.imread('../other/graphs/all_price_correlation_1m.png')
plt.figure(figsize=(10, 6))
plt.imshow(img)
plt.axis('off')
plt.show()

```

To solve this issue, "open_diff," "high_diff," "low_diff," and "close_diff" were added to include the lagged differences in the prices. Lagged difference refers to the difference in prices from the current price to the price from the previous timestamp. They are often used in predictive models in financial time series to solve the multicollinearity issue with the original prices, as suggested in [@moews2019lagged]. In @fig-figure7 and @fig-figure8 below, the correlation between the lagged difference price variables (open_diff, high_diff, low_diff, and close_diff) were displayed.

```{python}

#| echo: false
#| label: fig-figure7
#| fig-cap: "All lagged price differences (open_diff, high_diff, low_diff, and close_diff) plotted for the 1-minute dataset."
#| warning: false
#| message: false


import matplotlib.pyplot as plt
import matplotlib.image as mpimg

img = mpimg.imread('../other/graphs/all_price_diff_plot_1m.png')
plt.figure(figsize=(10, 6))
plt.imshow(img)
plt.axis('off')
plt.show()

```

```{python}

#| echo: false
#| label: fig-figure8
#| fig-cap: "Correlation matrix (open_diff, high_diff, low_diff, and close_diff) plotted for the 1-minute dataset."
#| warning: false
#| message: false


import matplotlib.pyplot as plt
import matplotlib.image as mpimg

img = mpimg.imread('../other/graphs/all_price_diff_correlation_1m.png')
plt.figure(figsize=(10, 6))
plt.imshow(img)
plt.axis('off')
plt.show()

```

# Models {#sec-models}

## Recurrent Neural Network (RNN) {#sec-rnn}
First published in [@elman1990finding] in 1990, RNN is a type of neural network that is designed to handle sequential data. Different from a feedforward neural network, RNN feeds the output from one step back as input for the next step, enabling RNN to "remember" information from the previous inputs, as described in [@GeeksforGeeks_2024]. This key mechanism makes RNN well-suited for time-series predictions and other sequential tasks. In this project, an RNN model was trained to predict Bitcoin's price movement direction for the 1-minute, 1-hour, and 1-day datasets. The features included lagged price differences (open_diff, high_diff, low_diff, close_diff), volume, a missing-data indicator (was_missing) for the 1-minute and 1-hour datasets, and the previous price movement direction (direction_t-1). The target variable was the future price movement direction (direction_t+1), which was coded into three classes: -1 (decrease), 0 (no change), and 1 (increase), as mentioned in @sec-featureengineering. In this project, the RNN has two simple RNN layers, with 128 and 64 nodes, respectively; a fully connected layer with 64 nodes; and the output layer uses softmax as its activation function, with three possible predictions: -1 (decrease), 0 (no change), and 1 (increase). Dropout was performed at all layers before the output layer to prevent overfitting. The loss function was cross-entropy.

## Long Short-Term Memory (LSTM) {#sec-lstm}
Originally published in [@hochreiter1997long] in 1997, LSTM is a special type of RNN, and it is designed to handle both short- and long-term dependencies in time series data, as stated in [@ghojogh2023recurrent]. Leaving out all the technical details, the primary function of LSTM is to retain important historical information, thus making it an excellent model for predicting sequential data, like financial forecasting and NLP. In this project, an LSTM model was trained to predict Bitcoin's price movement direction at different time intervals. Using the LSTM trained on the 1-minute dataset as an example, the features used for prediction are the lagged differences of price variables (open_diff, high_diff, low_diff, close_diff), volume, the missing-data indicator (was_missing), and the direction of the price movement at the previous timestamp (direction_t-1). These features were normalized using a MinMaxScaler offered in [@pedregosa2011scikit] to improve the model's performance. The dataset was transformed into sequences with a time step of 10 (10 observations in 10 minutes) to allow LSTM to learn short-term patterns over short historical periods. As for the model itself, the LSTM model has one LSTM layer with 128 nodes with a ReLU activation function, two fully connected layers with 64 nodes per layer with random dropout to prevent overfitting, and a binary output layer with a sigmoid activation function. The loss function was cross-entropy.

## Extreme Gradient Boosting (XGB) {#sec-xgb}
According to [@chen2016xgboost], XGB is a highly optimized and efficient implementation of the gradient boosting framework. In [@shwartz2022tabular], the authors stated that "XGBoost is widely used for structured and tabular data due to its flexibility, performance, and ability to handle missing data effectively." Despite not being explicitly developed for sequential data, XGB is a commonly used machine learning model for both regression and classification with excellent performance, according to [@Mukherjee_2023]. In this project, XGB classifiers were used to predict Bitcoin's price movement direction for all three datasets. Unlike the previous two sequential models (LSTM and RNN), XGB works on tabular data without requiring sequential dependencies. The features included lagged price differences (open_diff, high_diff, low_diff, close_diff), volume, a missing-data indicator (was_missing) for the 1-minute and 1-hour datasets, and the previous price movement direction (direction_t-1). The target variable was the future price movement direction (direction_t+1), which was coded into three classes: -1 (decrease), 0 (no change), and 1 (increase), as mentioned in @sec-featureengineering. The XGB model itself has 100 estimators to balance training speed and model complexity, a maximum depth of 10, and a 0.1 learning rate.

# Results {#sec-results}

## Performance 
This subsection compares the performance of the three models across each dataset. Key metrics, including accuracy, F1-score, and confusion matrices, are used to evaluate and contrast their effectiveness.

### RNN
RNN performs the worst when it is applied to predict a 1-minute interval dataset. The overall accuracy is only 0.41, and the F-1 scores are 0.51, 0.20, and 0.35, respectively, for the prediction of a decrease, no change, and an increase. When used on the 1-hour dataset, RNN performs significantly better, with an overall 0.52 accuracy and 0.52 F-1 scores for both increase and decrease. Lastly, RNN also performs well on the 1-day dataset, with an overall 0.52 accuracy and 0.32 and 0.63 F-1 scores for predicting a decrease and increase. The confusion matrixes of RNN on these three different time intervals are shown in @fig-figure9, @fig-figure10, and @fig-figure11.

```{python}

#| echo: false
#| label: fig-figure9
#| fig-cap: "Confusion matrix for RNN on 1-minute dataset"
#| warning: false
#| message: false


import matplotlib.pyplot as plt
import matplotlib.image as mpimg

img = mpimg.imread('../other/graphs/rnn_confusion_1m.png')
plt.figure(figsize=(10, 6))
plt.imshow(img)
plt.axis('off')
plt.show()

```

```{python}

#| echo: false
#| label: fig-figure10
#| fig-cap: "Confusion matrix for RNN on 1-hour dataset"
#| warning: false
#| message: false


import matplotlib.pyplot as plt
import matplotlib.image as mpimg

img = mpimg.imread('../other/graphs/rnn_confusion_1h.png')
plt.figure(figsize=(10, 6))
plt.imshow(img)
plt.axis('off')
plt.show()

```

```{python}

#| echo: false
#| label: fig-figure11
#| fig-cap: "Confusion matrix for RNN on 1-day dataset"
#| warning: false
#| message: false


import matplotlib.pyplot as plt
import matplotlib.image as mpimg

img = mpimg.imread('../other/graphs/rnn_confusion_1d.png')
plt.figure(figsize=(10, 6))
plt.imshow(img)
plt.axis('off')
plt.show()

```

### LSTM
LSTM did not work as intended. All of them only predict a decrease for the next timestamp. For example, for the 1-minute dataset, LSTM produced an overall accuracy of 0.48, and its confusion matrix is shown in @fig-figure12. The other statistics will not be provided here in detail because they are useless, considering the model is not making any meaningful prediction. For the details of LSTM models, please read about them in the actual model script, called "06-model_data.ipynb", located under the "scripts" folder.

```{python}

#| echo: false
#| label: fig-figure12
#| fig-cap: "Confusion matrix for LSTM on 1-hour dataset"
#| warning: false
#| message: false


import matplotlib.pyplot as plt
import matplotlib.image as mpimg

img = mpimg.imread('../other/graphs/lstm_confusion_1h.png')
plt.figure(figsize=(10, 6))
plt.imshow(img)
plt.axis('off')
plt.show()

```

### XGB
Opposite to RNN, XGB performs well when it is applied to predict a 1-minute interval dataset. The overall accuracy is only 0.53, and the F-1 scores are 0.58 and 0.46 for the prediction of a decrease and an increase. When used on the 1-hour dataset, XGB produced an overall 0.53 accuracy and 0.51 and 0.55 for the prediction of a decrease and an increase. Lastly, XGB performs slightly worse on the 1-day dataset, with an overall 0.50 accuracy and 0.49 and 0.52 F-1 scores for predicting a decrease and increase. The confusion matrixes of RNN on these three different time intervals are shown in @fig-figure13, @fig-figure14, and @fig-figure15.

```{python}

#| echo: false
#| label: fig-figure13
#| fig-cap: "Confusion matrix for XGB on 1-minute dataset"
#| warning: false
#| message: false


import matplotlib.pyplot as plt
import matplotlib.image as mpimg

img = mpimg.imread('../other/graphs/XGB_confusion_1m.png')
plt.figure(figsize=(10, 6))
plt.imshow(img)
plt.axis('off')
plt.show()

```
```{python}

#| echo: false
#| label: fig-figure14
#| fig-cap: "Confusion matrix for XGB on 1-hour dataset"
#| warning: false
#| message: false


import matplotlib.pyplot as plt
import matplotlib.image as mpimg

img = mpimg.imread('../other/graphs/XGB_confusion_1h.png')
plt.figure(figsize=(10, 6))
plt.imshow(img)
plt.axis('off')
plt.show()

```

```{python}

#| echo: false
#| label: fig-figure15
#| fig-cap: "Confusion matrix for XGB on 1-day dataset"
#| warning: false
#| message: false


import matplotlib.pyplot as plt
import matplotlib.image as mpimg

img = mpimg.imread('../other/graphs/XGB_confusion_1d.png')
plt.figure(figsize=(10, 6))
plt.imshow(img)
plt.axis('off')
plt.show()

```

### Model Performance Comparison
Among the three tested models, XGB produced the best overall accuracy, especially when predicting on the 1-minute and 1-hour interval datasets, showing its strength in handling the data effectively. In contrast, LSTM performed the worst, failing to make meaningful predictions and only predicting a decrease for all instances. This limited its performance in this study. While RNN struggled with the 1-minute dataset, it performed better on the 1-hour and 1-day datasets and even achieved the highest F1-score of 0.63 for predicting an increase on the 1-day dataset. This suggests that RNN may be better suited for datasets with larger time intervals, where sequential dependencies are more influential. Overall, the results indicate that XGB offers a more reliable balance of performance and efficiency. In contrast, the sequential models (RNN and LSTM) require further optimization or adjustments to deliver competitive results.

## Training Effort
XGB trains the fastest, followed by RNN, with LSTM taking the longest to train when using only a CPU. This project used a 16-core Intel Core i7-10700KF 3.80GHz CPU to train all the models. Using the 1-minute dataset as an example, XGB took only 8 minutes to complete training, RNN took 2 and 40 minutes, while LSTM used 10 and 31 minutes. These results confirmed the efficiency of XGB, which is particularly well-suited for individuals with limited computational resources. RNN, though slower, demonstrates a reasonable trade-off between training time and predictive performance, particularly for datasets with longer intervals.
On the other hand, LSTM has the most prolonged training duration and least optimal performance. This suggests that when training LSTM models, better optimization or alternative approaches can be beneficial without access to GPUs or distributed computing resources. This extreme difference in training times demonstrates the practical implications of model selection for computationally constrained traders.

# Discussion {#sec-discussion}

\newpage

# References